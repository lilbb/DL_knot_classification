{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"GYBIkndtz2Fu"},"outputs":[],"source":["# This part uses visualization to explain the EfficientNet model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23766,"status":"ok","timestamp":1690683170876,"user":{"displayName":"lilbb","userId":"00559334782067035222"},"user_tz":-60},"id":"1Susd9j60B2b","outputId":"7cac9c8c-df2d-4eaa-cb33-80c1a58235d6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting timm\n","  Downloading timm-0.9.2-py3-none-any.whl (2.2 MB)\n","\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/2.2 MB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m39.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m30.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: torch>=1.7 in /usr/local/lib/python3.10/dist-packages (from timm) (2.0.1+cu118)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.15.2+cu118)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.1)\n","Collecting huggingface-hub (from timm)\n","  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m32.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors (from timm)\n","  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m72.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.12.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (4.7.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.7->timm) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.7->timm) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.7->timm) (16.0.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (2023.6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (2.27.1)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (4.65.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->timm) (23.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (1.22.4)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (9.4.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.7->timm) (2.1.3)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (1.26.16)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (2023.7.22)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->timm) (3.4)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.7->timm) (1.3.0)\n","Installing collected packages: safetensors, huggingface-hub, timm\n","Successfully installed huggingface-hub-0.16.4 safetensors-0.3.1 timm-0.9.2\n","Collecting optuna\n","  Downloading optuna-3.2.0-py3-none-any.whl (390 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m390.6/390.6 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting alembic>=1.5.0 (from optuna)\n","  Downloading alembic-1.11.1-py3-none-any.whl (224 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.5/224.5 kB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting cmaes>=0.9.1 (from optuna)\n","  Downloading cmaes-0.10.0-py3-none-any.whl (29 kB)\n","Collecting colorlog (from optuna)\n","  Downloading colorlog-6.7.0-py2.py3-none-any.whl (11 kB)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from optuna) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (23.1)\n","Requirement already satisfied: sqlalchemy>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from optuna) (2.0.19)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from optuna) (4.65.0)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from optuna) (6.0.1)\n","Collecting Mako (from alembic>=1.5.0->optuna)\n","  Downloading Mako-1.2.4-py3-none-any.whl (78 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.7/78.7 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: typing-extensions>=4 in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna) (4.7.1)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.3.0->optuna) (2.0.2)\n","Requirement already satisfied: MarkupSafe>=0.9.2 in /usr/local/lib/python3.10/dist-packages (from Mako->alembic>=1.5.0->optuna) (2.1.3)\n","Installing collected packages: Mako, colorlog, cmaes, alembic, optuna\n","Successfully installed Mako-1.2.4 alembic-1.11.1 cmaes-0.10.0 colorlog-6.7.0 optuna-3.2.0\n","Collecting pytorch-gradcam\n","  Downloading pytorch-gradcam-0.2.1.tar.gz (6.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m47.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from pytorch-gradcam) (4.7.0.72)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from pytorch-gradcam) (1.22.4)\n","Building wheels for collected packages: pytorch-gradcam\n","  Building wheel for pytorch-gradcam (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pytorch-gradcam: filename=pytorch_gradcam-0.2.1-py3-none-any.whl size=5248 sha256=bc61ba5042da4cb15ca33df3b8fca217998edecb8d1dd104c57390dd6697d19a\n","  Stored in directory: /root/.cache/pip/wheels/6f/f1/8f/96c81d13f617841f23cae192a77fea3e9e988d058ba9414f2c\n","Successfully built pytorch-gradcam\n","Installing collected packages: pytorch-gradcam\n","Successfully installed pytorch-gradcam-0.2.1\n","Collecting captum\n","  Downloading captum-0.6.0-py3-none-any.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m17.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from captum) (3.7.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from captum) (1.22.4)\n","Requirement already satisfied: torch>=1.6 in /usr/local/lib/python3.10/dist-packages (from captum) (2.0.1+cu118)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (3.12.2)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (4.7.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6->captum) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6->captum) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6->captum) (16.0.6)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (1.1.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (0.11.0)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (4.41.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (1.4.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (23.1)\n","Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (9.4.0)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (3.1.0)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->captum) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->captum) (1.16.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6->captum) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6->captum) (1.3.0)\n","Installing collected packages: captum\n","Successfully installed captum-0.6.0\n"]}],"source":["!pip install timm\n","!pip install optuna\n","!pip install pytorch-gradcam\n","!pip install captum"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1vmKpwDEYnwZPu9CDP-pOiTVm5zVJx10d"},"executionInfo":{"elapsed":128103,"status":"ok","timestamp":1690676635479,"user":{"displayName":"lilbb","userId":"00559334782067035222"},"user_tz":-60},"id":"5S-YDyABz_jm","outputId":"1b185423-de3e-4684-c8b3-f715b0a6607a"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["# Use EfficientNet(tf_efficientnet_b5_ns) draw the curves with best hyperparameters\n","import os\n","import numpy as np\n","import pandas as pd\n","from PIL import Image\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","from google.colab import drive\n","import torch\n","from torchvision import transforms\n","from torchvision.datasets import ImageFolder\n","from torch.utils.data import DataLoader, random_split\n","from torchvision import models\n","import torch.nn as nn\n","import torch.optim as optim\n","import time\n","import copy\n","from torch.utils.data import Dataset\n","from torchvision.io import read_image\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import f1_score, confusion_matrix\n","import seaborn as sns\n","import random\n","import timm\n","from gradcam.utils import visualize_cam\n","from gradcam import GradCAM, GradCAMpp\n","\n","# Connect to the google conlab\n","drive.mount('/content/drive')\n","\n","# Use transforms to do data augmentation and preprocess\n","data_transforms = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.RandomRotation(45),  # Rotate randomly between 0 and 45 degrees\n","    transforms.RandomHorizontalFlip(),  # Random horizontal flip\n","    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),  # Randomly crop and scale to 224x224 size\n","    transforms.Grayscale(num_output_channels=3),  # Grayscale\n","    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),  # Normalization\n","])\n","\n","class Knots_Dataset(Dataset):\n","    def __init__(self, img_dir, transform=None, target_transform=None):\n","        self.img_dir = img_dir\n","        self.transform = transform\n","        self.target_transform = target_transform\n","        self.data = []\n","        self.labels = []\n","        self.label_encoder = LabelEncoder()\n","\n","        # Loop over all conditions\n","        for knot_type in os.listdir(img_dir):\n","            for light_condition in os.listdir(os.path.join(img_dir, knot_type)):\n","                for tension_condition in os.listdir(os.path.join(img_dir, knot_type, light_condition)):\n","                    images = os.listdir(os.path.join(img_dir, knot_type, light_condition, tension_condition))\n","                    random.shuffle(images)  # Shuffle the images\n","                    for image in images:\n","                        self.data.append(os.path.join(img_dir, knot_type, light_condition, tension_condition, image))\n","                        self.labels.append(knot_type)\n","\n","        # Fit the label encoder and transform the labels\n","        self.labels = self.label_encoder.fit_transform(self.labels)\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        img_path = self.data[idx]\n","        image = Image.open(img_path)\n","        image = image.convert('RGB')\n","        label = self.labels[idx]\n","        if self.transform:\n","            image = self.transform(image)\n","        if self.target_transform:\n","            label = self.target_transform(label)\n","        return image, label\n","\n","# Create a custom data set\n","dataset = Knots_Dataset(\"/content/drive/My Drive/MSc Project/rawdata\", transform=data_transforms)\n","\n","# Split the dataset into training set and validation set\n","train_dataset = []\n","val_dataset = []\n","for i in range(len(dataset)):\n","    if i % 4 == 0:  # Put 25% of the images in the validation set\n","        val_dataset.append(dataset[i])\n","    else:  # Put the rest in the training set\n","        train_dataset.append(dataset[i])\n","\n","# Create a Dataloader\n","train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=True)\n","\n","dataloaders = {'train': train_dataloader, 'val': val_dataloader}\n","dataset_sizes = {'train': len(train_dataset), 'val': len(val_dataset)}\n","\n","# Check if model can be trained on GPU\n","if torch.cuda.is_available():\n","    device = torch.device(\"cuda\")\n","    print(\"Runing on GPU\")\n","    print(\"CUDA device count: \", torch.cuda.device_count())\n","else:\n","    device = torch.device(\"cpu\")\n","    print(\"Runing on CPU\")\n","\n","# Save the model results\n","train_losses, train_accuracies = [], []\n","val_losses, val_accuracies = [], []\n","train_f1_scores, val_f1_scores = [], []\n","\n","def train_model(model, criterion, optimizer, num_epochs=25):\n","    since = time.time()\n","\n","    best_model_wts = copy.deepcopy(model.state_dict())\n","    best_acc = 0.0\n","    best_preds = None\n","    best_labels = None\n","\n","    for epoch in range(num_epochs):\n","        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n","        print('-' * 10)\n","\n","        # Each epoch has a training and validation phase\n","        for phase in ['train', 'val']:\n","            if phase == 'train':\n","                model.train()  # Set model to training mode\n","            else:\n","                model.eval()   # Set model to evaluate mode\n","\n","            running_loss = 0.0\n","            running_corrects = 0\n","            running_preds = []\n","            running_labels = []\n","\n","            # Iterate over data\n","            for inputs, labels in dataloaders[phase]:\n","                inputs, labels = inputs.to(device), labels.to(device)\n","\n","                # zero(clean) the parameter gradients\n","                optimizer.zero_grad()\n","\n","                # forward\n","                # track history if only in train\n","                with torch.set_grad_enabled(phase == 'train'):\n","                    outputs = model(inputs)\n","                    _, preds = torch.max(outputs, 1)\n","                    loss = criterion(outputs, labels)\n","\n","                    # backward + optimize only if in training phase\n","                    if phase == 'train':\n","                        loss.backward()\n","                        optimizer.step()\n","\n","                # statistics\n","                running_loss += loss.item() * inputs.size(0)\n","                running_corrects += torch.sum(preds == labels.data)\n","                running_preds.extend(preds.cpu().numpy())\n","                running_labels.extend(labels.cpu().numpy())\n","\n","            epoch_loss = running_loss / dataset_sizes[phase]\n","            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n","            epoch_acc = epoch_acc.cpu().numpy()  # Move tensor to CPU and convert to numpy(debug)\n","            epoch_f1_score = f1_score(running_labels, running_preds, average='macro')\n","            epoch_f1_score = torch.tensor(epoch_f1_score).cpu().numpy()  # Move tensor to CPU and convert to numpy(debug)\n","\n","            if phase == 'train':\n","                train_losses.append(epoch_loss)\n","                train_accuracies.append(epoch_acc)\n","                train_f1_scores.append(epoch_f1_score)\n","            else:\n","                val_losses.append(epoch_loss)\n","                val_accuracies.append(epoch_acc)\n","                val_f1_scores.append(epoch_f1_score)\n","\n","            print('{} Loss: {:.5f}  Acc: {:.5f}   F1_Score: {:.5f}'.format(\n","                phase, epoch_loss, epoch_acc, epoch_f1_score))\n","\n","            # deep copy the model\n","            if phase == 'val' and epoch_acc > best_acc:\n","                best_acc = epoch_acc\n","                best_model_wts = copy.deepcopy(model.state_dict())\n","                best_preds = running_preds\n","                best_labels = running_labels\n","\n","        print()\n","\n","    time_elapsed = time.time() - since\n","    print('Training complete in {:.0f}m {:.0f}s'.format(\n","        time_elapsed // 60, time_elapsed % 60))\n","    print('Best val Acc: {:5f}'.format(best_acc))\n","\n","    # load best model weights\n","    model.load_state_dict(best_model_wts)\n","\n","    return model, best_preds, best_labels\n","\n","\n","\n","# [I 2023-07-18 19:35:10,874] Trial 49 finished with value: 0.9851851851851853\n","# and parameters: {'lr': 0.00015305939437836783, 'optimizer': 'RMSprop', 'weight_decay': 2.0510689201040282e-10}. Best is trial 49 with value: 0.9851851851851853.\n","# Use the best hyperparameters to train the model\n","best_lr = 0.00001504939437836783  # 0.00001104939437836783\n","# 0.00001705939437836783 Best val Acc: 0.955556\n","# 0.00001103939437836783 Best val Acc: 0.955556\n","# 0.00001104939437836783 Best val Acc: 0.959259\n","# 0.00001104939637836783 Best val Acc: 0.970370\n","best_optimizer_name = 'RMSprop'\n","best_weight_decay = 2.0510689201040282e-10\n","\n","# Load a pretrained model(EfficientNet)\n","model = timm.create_model('tf_efficientnet_b5_ns', pretrained=True)\n","num_ftrs = model.classifier.in_features\n","model.classifier = nn.Linear(num_ftrs, 5)  # Change the classifier layer to 5\n","model = model.to(device)\n","\n","criterion = nn.CrossEntropyLoss()\n","\n","# Use the best optimizer\n","if best_optimizer_name == 'SGD':\n","    optimizer = optim.SGD(model.parameters(), lr=best_lr, weight_decay=best_weight_decay)\n","elif best_optimizer_name == 'Adam':\n","    optimizer = optim.Adam(model.parameters(), lr=best_lr, weight_decay=best_weight_decay)\n","else:\n","    optimizer = optim.RMSprop(model.parameters(), lr=best_lr, weight_decay=best_weight_decay)\n","\n","\n","# Train and evaluate the best model\n","model, best_preds, best_labels = train_model(model, criterion, optimizer, num_epochs=25)\n","\n","\n","\n","\n","\n","\n","\n","\n","# Define a function to apply GradCAM to a given image\n","def apply_gradcam(img, label):\n","    # Add batch dimension and move to device\n","    img = img.unsqueeze(0).to(device)\n","\n","    # Set the model to evaluation mode and get the output\n","    model.eval()\n","    out = model(img)\n","\n","    # Initialize GradCAM\n","    gradcam = GradCAM(model, model.conv_head)\n","\n","    # Get the GradCAM heatmap\n","    mask, _ = gradcam(img)\n","    heatmap, result = visualize_cam(mask, img)\n","\n","    # Plot original image and heatmap\n","    fig, axarr = plt.subplots(nrows=1, ncols=2)\n","    img = img.squeeze().cpu().numpy().transpose(1, 2, 0)  # Squeeze before transpose\n","    img = (img - img.min()) / (img.max() - img.min())\n","    axarr[0].imshow(img)\n","    heatmap = heatmap.squeeze().cpu().numpy().transpose(1, 2, 0)  # Squeeze and transpose before displaying\n","    axarr[1].imshow(heatmap)\n","    plt.show()\n","\n","\n","\n","\n","\n","# Loop over all conditions\n","for knot_type in os.listdir(\"/content/drive/My Drive/MSc Project/rawdata\"):\n","    for light_condition in os.listdir(os.path.join(\"/content/drive/My Drive/MSc Project/rawdata\", knot_type)):\n","        for tension_condition in os.listdir(os.path.join(\"/content/drive/My Drive/MSc Project/rawdata\", knot_type, light_condition)):\n","            # Get the first image of this condition\n","            image_file = os.listdir(os.path.join(\"/content/drive/My Drive/MSc Project/rawdata\", knot_type, light_condition, tension_condition))[0]\n","            img_path = os.path.join(\"/content/drive/My Drive/MSc Project/rawdata\", knot_type, light_condition, tension_condition, image_file)\n","            image = Image.open(img_path)\n","            image = image.convert('RGB')\n","            image = data_transforms(image)\n","            label = knot_type\n","\n","            print(f\"Applying GradCAM for knot type {knot_type}, light condition {light_condition}, tension condition {tension_condition}\")\n","            apply_gradcam(image, label)\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","# Define knot types\n","knot_types = ['Bowline', 'Clove_Hitch', 'Figure_8_Knot', 'Overhand', 'Reef']\n","\n","# Plot confusion matrix\n","confusion_mtx = confusion_matrix(best_labels, best_preds)\n","plt.figure(figsize=(10, 8))\n","sns.heatmap(confusion_mtx, annot=True, fmt='d', cmap='Blues', xticklabels=knot_types, yticklabels=knot_types)\n","plt.xlabel('Prediction')\n","plt.ylabel('True')\n","plt.title('Confusion Matrix')\n","plt.show()\n","\n","\n","\n","# Plot loss curves\n","plt.figure()\n","plt.plot(train_losses, label='Train')\n","plt.plot(val_losses, label='Validation')\n","plt.title('Loss Curves')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.show()\n","\n","# Plot accuracy curves\n","plt.figure()\n","plt.plot(train_accuracies, label='Train')\n","plt.plot(val_accuracies, label='Validation')\n","plt.title('Accuracy Curves')\n","plt.xlabel('Epochs')\n","plt.ylabel('Accuracy')\n","plt.legend()\n","plt.show()\n","\n","# Plot F1 Score curves\n","plt.figure()\n","plt.plot(train_f1_scores, label='Train')\n","plt.plot(val_f1_scores, label='Validation')\n","plt.title('F1 Score Curves')\n","plt.xlabel('Epochs')\n","plt.ylabel('F1 Score')\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1jtlDMaGdFEI4OW6nozyhQNEFjutSRvGR"},"executionInfo":{"elapsed":139989,"status":"ok","timestamp":1690679001808,"user":{"displayName":"lilbb","userId":"00559334782067035222"},"user_tz":-60},"id":"kdBJ17Os_XFk","outputId":"5f64ac30-41ef-4786-863d-df85cb6beb02"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["# Use EfficientNet(tf_efficientnet_b5_ns) draw the curves with best hyperparameters\n","import os\n","import numpy as np\n","import pandas as pd\n","from PIL import Image\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","from google.colab import drive\n","import torch\n","from torchvision import transforms\n","from torchvision.datasets import ImageFolder\n","from torch.utils.data import DataLoader, random_split\n","from torchvision import models\n","import torch.nn as nn\n","import torch.optim as optim\n","import time\n","import copy\n","from torch.utils.data import Dataset\n","from torchvision.io import read_image\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import f1_score, confusion_matrix\n","import seaborn as sns\n","import random\n","import timm\n","from gradcam.utils import visualize_cam\n","from gradcam import GradCAM, GradCAMpp\n","from captum.attr import IntegratedGradients\n","from captum.attr import visualization as viz\n","from captum.attr import Occlusion\n","\n","# Connect to the google conlab\n","drive.mount('/content/drive')\n","\n","default_cmap = plt.cm.viridis\n","\n","# Use transforms to do data augmentation and preprocess\n","data_transforms = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.RandomRotation(45),  # Rotate randomly between 0 and 45 degrees\n","    transforms.RandomHorizontalFlip(),  # Random horizontal flip\n","    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),  # Randomly crop and scale to 224x224 size\n","    transforms.Grayscale(num_output_channels=3),  # Grayscale\n","    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),  # Normalization\n","])\n","\n","class Knots_Dataset(Dataset):\n","    def __init__(self, img_dir, transform=None, target_transform=None):\n","        self.img_dir = img_dir\n","        self.transform = transform\n","        self.target_transform = target_transform\n","        self.data = []\n","        self.labels = []\n","        self.label_encoder = LabelEncoder()\n","\n","        # Loop over all conditions\n","        for knot_type in os.listdir(img_dir):\n","            for light_condition in os.listdir(os.path.join(img_dir, knot_type)):\n","                for tension_condition in os.listdir(os.path.join(img_dir, knot_type, light_condition)):\n","                    images = os.listdir(os.path.join(img_dir, knot_type, light_condition, tension_condition))\n","                    random.shuffle(images)  # Shuffle the images\n","                    for image in images:\n","                        self.data.append(os.path.join(img_dir, knot_type, light_condition, tension_condition, image))\n","                        self.labels.append(knot_type)\n","\n","        # Fit the label encoder and transform the labels\n","        self.labels = self.label_encoder.fit_transform(self.labels)\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        img_path = self.data[idx]\n","        image = Image.open(img_path)\n","        image = image.convert('RGB')\n","        label = self.labels[idx]\n","        if self.transform:\n","            image = self.transform(image)\n","        if self.target_transform:\n","            label = self.target_transform(label)\n","        return image, label\n","\n","    def get_label_int(self, label_str):\n","        return self.label_encoder.transform([label_str])[0]\n","\n","# Create a custom data set\n","dataset = Knots_Dataset(\"/content/drive/My Drive/MSc Project/rawdata\", transform=data_transforms)\n","\n","# Split the dataset into training set and validation set\n","train_dataset = []\n","val_dataset = []\n","for i in range(len(dataset)):\n","    if i % 4 == 0:  # Put 25% of the images in the validation set\n","        val_dataset.append(dataset[i])\n","    else:  # Put the rest in the training set\n","        train_dataset.append(dataset[i])\n","\n","# Create a Dataloader\n","train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=True)\n","\n","dataloaders = {'train': train_dataloader, 'val': val_dataloader}\n","dataset_sizes = {'train': len(train_dataset), 'val': len(val_dataset)}\n","\n","# Check if model can be trained on GPU\n","if torch.cuda.is_available():\n","    device = torch.device(\"cuda\")\n","    print(\"Runing on GPU\")\n","    print(\"CUDA device count: \", torch.cuda.device_count())\n","else:\n","    device = torch.device(\"cpu\")\n","    print(\"Runing on CPU\")\n","\n","# Save the model results\n","train_losses, train_accuracies = [], []\n","val_losses, val_accuracies = [], []\n","train_f1_scores, val_f1_scores = [], []\n","\n","def train_model(model, criterion, optimizer, num_epochs=25):\n","    since = time.time()\n","\n","    best_model_wts = copy.deepcopy(model.state_dict())\n","    best_acc = 0.0\n","    best_preds = None\n","    best_labels = None\n","\n","    for epoch in range(num_epochs):\n","        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n","        print('-' * 10)\n","\n","        # Each epoch has a training and validation phase\n","        for phase in ['train', 'val']:\n","            if phase == 'train':\n","                model.train()  # Set model to training mode\n","            else:\n","                model.eval()   # Set model to evaluate mode\n","\n","            running_loss = 0.0\n","            running_corrects = 0\n","            running_preds = []\n","            running_labels = []\n","\n","            # Iterate over data\n","            for inputs, labels in dataloaders[phase]:\n","                inputs, labels = inputs.to(device), labels.to(device)\n","\n","                # zero(clean) the parameter gradients\n","                optimizer.zero_grad()\n","\n","                # forward\n","                # track history if only in train\n","                with torch.set_grad_enabled(phase == 'train'):\n","                    outputs = model(inputs)\n","                    _, preds = torch.max(outputs, 1)\n","                    loss = criterion(outputs, labels)\n","\n","                    # backward + optimize only if in training phase\n","                    if phase == 'train':\n","                        loss.backward()\n","                        optimizer.step()\n","\n","                # statistics\n","                running_loss += loss.item() * inputs.size(0)\n","                running_corrects += torch.sum(preds == labels.data)\n","                running_preds.extend(preds.cpu().numpy())\n","                running_labels.extend(labels.cpu().numpy())\n","\n","            epoch_loss = running_loss / dataset_sizes[phase]\n","            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n","            epoch_acc = epoch_acc.cpu().numpy()  # Move tensor to CPU and convert to numpy(debug)\n","            epoch_f1_score = f1_score(running_labels, running_preds, average='macro')\n","            epoch_f1_score = torch.tensor(epoch_f1_score).cpu().numpy()  # Move tensor to CPU and convert to numpy(debug)\n","\n","            if phase == 'train':\n","                train_losses.append(epoch_loss)\n","                train_accuracies.append(epoch_acc)\n","                train_f1_scores.append(epoch_f1_score)\n","            else:\n","                val_losses.append(epoch_loss)\n","                val_accuracies.append(epoch_acc)\n","                val_f1_scores.append(epoch_f1_score)\n","\n","            print('{} Loss: {:.5f}  Acc: {:.5f}   F1_Score: {:.5f}'.format(\n","                phase, epoch_loss, epoch_acc, epoch_f1_score))\n","\n","            # deep copy the model\n","            if phase == 'val' and epoch_acc > best_acc:\n","                best_acc = epoch_acc\n","                best_model_wts = copy.deepcopy(model.state_dict())\n","                best_preds = running_preds\n","                best_labels = running_labels\n","\n","        print()\n","\n","    time_elapsed = time.time() - since\n","    print('Training complete in {:.0f}m {:.0f}s'.format(\n","        time_elapsed // 60, time_elapsed % 60))\n","    print('Best val Acc: {:5f}'.format(best_acc))\n","\n","    # load best model weights\n","    model.load_state_dict(best_model_wts)\n","\n","    return model, best_preds, best_labels\n","\n","\n","\n","# [I 2023-07-18 19:35:10,874] Trial 49 finished with value: 0.9851851851851853\n","# and parameters: {'lr': 0.00015305939437836783, 'optimizer': 'RMSprop', 'weight_decay': 2.0510689201040282e-10}. Best is trial 49 with value: 0.9851851851851853.\n","# Use the best hyperparameters to train the model\n","best_lr = 0.00001504939437836783  # 0.00001104939437836783\n","# 0.00001705939437836783 Best val Acc: 0.955556\n","# 0.00001103939437836783 Best val Acc: 0.955556\n","# 0.00001104939437836783 Best val Acc: 0.959259\n","# 0.00001104939637836783 Best val Acc: 0.970370\n","best_optimizer_name = 'RMSprop'\n","best_weight_decay = 2.0510689201040282e-10\n","\n","# Load a pretrained model(EfficientNet)\n","model = timm.create_model('tf_efficientnet_b5_ns', pretrained=True)\n","num_ftrs = model.classifier.in_features\n","model.classifier = nn.Linear(num_ftrs, 5)  # Change the classifier layer to 5\n","model = model.to(device)\n","\n","criterion = nn.CrossEntropyLoss()\n","\n","# Use the best optimizer\n","if best_optimizer_name == 'SGD':\n","    optimizer = optim.SGD(model.parameters(), lr=best_lr, weight_decay=best_weight_decay)\n","elif best_optimizer_name == 'Adam':\n","    optimizer = optim.Adam(model.parameters(), lr=best_lr, weight_decay=best_weight_decay)\n","else:\n","    optimizer = optim.RMSprop(model.parameters(), lr=best_lr, weight_decay=best_weight_decay)\n","\n","\n","# Train and evaluate the best model\n","model, best_preds, best_labels = train_model(model, criterion, optimizer, num_epochs=25)\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","# Define a function to apply Integrated Gradients to a given image\n","def apply_integrated_gradients(img, label):\n","    # Convert label from string to int\n","    label_to_int = {'Bowline': 0, 'Clove_Hitch': 1, 'Figure_8_Knot': 2, 'Overhand': 3, 'Reef': 4}\n","    label = label_to_int[label]\n","\n","    # Add batch dimension and move to device\n","    img = img.unsqueeze(0).to(device)\n","\n","    # Set the model to evaluation mode and get the output\n","    model.eval()\n","    out = model(img)\n","\n","    # Initialize IntegratedGradients object\n","    ig = IntegratedGradients(model)\n","\n","    # Compute attribution scores\n","    attributions_ig = ig.attribute(img, target=label)\n","\n","    # Plot original image and heatmap\n","    fig, axarr = plt.subplots(nrows=1, ncols=2)\n","    img = img.squeeze().cpu().numpy().transpose(1, 2, 0)  # Squeeze before transpose\n","    img = (img - img.min()) / (img.max() - img.min())\n","    axarr[0].imshow(img)\n","    attributions_ig = attributions_ig.squeeze().cpu().numpy().transpose(1, 2, 0)\n","    attributions_ig = (attributions_ig - attributions_ig.min()) / (attributions_ig.max() - attributions_ig.min())\n","    axarr[1].imshow(attributions_ig)\n","    plt.show()\n","\n","\n","# Loop over all conditions\n","for knot_type in os.listdir(\"/content/drive/My Drive/MSc Project/rawdata\"):\n","    for light_condition in os.listdir(os.path.join(\"/content/drive/My Drive/MSc Project/rawdata\", knot_type)):\n","        for tension_condition in os.listdir(os.path.join(\"/content/drive/My Drive/MSc Project/rawdata\", knot_type, light_condition)):\n","            # Get the first image of this condition\n","            image_file = os.listdir(os.path.join(\"/content/drive/My Drive/MSc Project/rawdata\", knot_type, light_condition, tension_condition))[0]\n","            img_path = os.path.join(\"/content/drive/My Drive/MSc Project/rawdata\", knot_type, light_condition, tension_condition, image_file)\n","            image = Image.open(img_path)\n","            image = image.convert('RGB')\n","            image = data_transforms(image)\n","            label = knot_type\n","\n","            print(f\"Applying Integrated Gradients for knot type {knot_type}, light condition {light_condition}, tension condition {tension_condition}\")\n","            apply_integrated_gradients(image, label)\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","# Define knot types\n","knot_types = ['Bowline', 'Clove_Hitch', 'Figure_8_Knot', 'Overhand', 'Reef']\n","\n","# Plot confusion matrix\n","confusion_mtx = confusion_matrix(best_labels, best_preds)\n","plt.figure(figsize=(10, 8))\n","sns.heatmap(confusion_mtx, annot=True, fmt='d', cmap='Blues', xticklabels=knot_types, yticklabels=knot_types)\n","plt.xlabel('Prediction')\n","plt.ylabel('True')\n","plt.title('Confusion Matrix')\n","plt.show()\n","\n","\n","\n","# Plot loss curves\n","plt.figure()\n","plt.plot(train_losses, label='Train')\n","plt.plot(val_losses, label='Validation')\n","plt.title('Loss Curves')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.show()\n","\n","# Plot accuracy curves\n","plt.figure()\n","plt.plot(train_accuracies, label='Train')\n","plt.plot(val_accuracies, label='Validation')\n","plt.title('Accuracy Curves')\n","plt.xlabel('Epochs')\n","plt.ylabel('Accuracy')\n","plt.legend()\n","plt.show()\n","\n","# Plot F1 Score curves\n","plt.figure()\n","plt.plot(train_f1_scores, label='Train')\n","plt.plot(val_f1_scores, label='Validation')\n","plt.title('F1 Score Curves')\n","plt.xlabel('Epochs')\n","plt.ylabel('F1 Score')\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"16dqAaJZlJjJECIHJbqgNN6u4ah7k3DTD"},"executionInfo":{"elapsed":138579,"status":"ok","timestamp":1690684874497,"user":{"displayName":"lilbb","userId":"00559334782067035222"},"user_tz":-60},"id":"ddElSvDsWkNM","outputId":"7afc389a-c9a1-4658-b178-d97df86c8ecc"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["# Visualize the wrong label\n","\n","# Use EfficientNet(tf_efficientnet_b5_ns) draw the curves with best hyperparameters\n","import os\n","import numpy as np\n","import pandas as pd\n","from PIL import Image\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelEncoder\n","from google.colab import drive\n","import torch\n","from torchvision import transforms\n","from torchvision.datasets import ImageFolder\n","from torch.utils.data import DataLoader, random_split\n","from torchvision import models\n","import torch.nn as nn\n","import torch.optim as optim\n","import time\n","import copy\n","from torch.utils.data import Dataset\n","from torchvision.io import read_image\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import f1_score, confusion_matrix\n","import seaborn as sns\n","import random\n","import timm\n","from gradcam.utils import visualize_cam\n","from gradcam import GradCAM, GradCAMpp\n","from captum.attr import IntegratedGradients\n","\n","\n","# Connect to the google conlab\n","drive.mount('/content/drive')\n","\n","default_cmap = plt.cm.viridis\n","\n","# Use transforms to do data augmentation and preprocess\n","data_transforms = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.RandomRotation(45),  # Rotate randomly between 0 and 45 degrees\n","    transforms.RandomHorizontalFlip(),  # Random horizontal flip\n","    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),  # Randomly crop and scale to 224x224 size\n","    transforms.Grayscale(num_output_channels=3),  # Grayscale\n","    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),  # Normalization\n","])\n","\n","class Knots_Dataset(Dataset):\n","    def __init__(self, img_dir, transform=None, target_transform=None):\n","        self.img_dir = img_dir\n","        self.transform = transform\n","        self.target_transform = target_transform\n","        self.data = []\n","        self.labels = []\n","        self.label_encoder = LabelEncoder()\n","\n","        # Loop over all conditions\n","        for knot_type in os.listdir(img_dir):\n","            for light_condition in os.listdir(os.path.join(img_dir, knot_type)):\n","                for tension_condition in os.listdir(os.path.join(img_dir, knot_type, light_condition)):\n","                    images = os.listdir(os.path.join(img_dir, knot_type, light_condition, tension_condition))\n","                    random.shuffle(images)  # Shuffle the images\n","                    for image in images:\n","                        self.data.append(os.path.join(img_dir, knot_type, light_condition, tension_condition, image))\n","                        self.labels.append(knot_type)\n","\n","        # Fit the label encoder and transform the labels\n","        self.labels = self.label_encoder.fit_transform(self.labels)\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        img_path = self.data[idx]\n","        image = Image.open(img_path)\n","        image = image.convert('RGB')\n","        label = self.labels[idx]\n","        if self.transform:\n","            image = self.transform(image)\n","        if self.target_transform:\n","            label = self.target_transform(label)\n","        return image, label\n","    def get_image_path(self, idx):\n","        return self.data[idx]\n","\n","\n","# Create a custom data set\n","dataset = Knots_Dataset(\"/content/drive/My Drive/MSc Project/rawdata\", transform=data_transforms)\n","\n","# Split the dataset into training set and validation set\n","train_dataset = []\n","val_dataset = []\n","for i in range(len(dataset)):\n","    if i % 4 == 0:  # Put 25% of the images in the validation set\n","        val_dataset.append(dataset[i])\n","    else:  # Put the rest in the training set\n","        train_dataset.append(dataset[i])\n","\n","# Create a Dataloader\n","train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n","val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=True)\n","\n","dataloaders = {'train': train_dataloader, 'val': val_dataloader}\n","dataset_sizes = {'train': len(train_dataset), 'val': len(val_dataset)}\n","\n","# Check if model can be trained on GPU\n","if torch.cuda.is_available():\n","    device = torch.device(\"cuda\")\n","    print(\"Runing on GPU\")\n","    print(\"CUDA device count: \", torch.cuda.device_count())\n","else:\n","    device = torch.device(\"cpu\")\n","    print(\"Runing on CPU\")\n","\n","# Save the model results\n","train_losses, train_accuracies = [], []\n","val_losses, val_accuracies = [], []\n","train_f1_scores, val_f1_scores = [], []\n","\n","def train_model(model, criterion, optimizer, num_epochs=25):\n","    since = time.time()\n","\n","    best_model_wts = copy.deepcopy(model.state_dict())\n","    best_acc = 0.0\n","    best_preds = None\n","    best_labels = None\n","\n","    for epoch in range(num_epochs):\n","        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n","        print('-' * 10)\n","\n","        # Each epoch has a training and validation phase\n","        for phase in ['train', 'val']:\n","            if phase == 'train':\n","                model.train()  # Set model to training mode\n","            else:\n","                model.eval()   # Set model to evaluate mode\n","\n","            running_loss = 0.0\n","            running_corrects = 0\n","            running_preds = []\n","            running_labels = []\n","\n","            # Iterate over data\n","            for inputs, labels in dataloaders[phase]:\n","                inputs, labels = inputs.to(device), labels.to(device)\n","\n","                # zero(clean) the parameter gradients\n","                optimizer.zero_grad()\n","\n","                # forward\n","                # track history if only in train\n","                with torch.set_grad_enabled(phase == 'train'):\n","                    outputs = model(inputs)\n","                    _, preds = torch.max(outputs, 1)\n","                    loss = criterion(outputs, labels)\n","\n","                    # backward + optimize only if in training phase\n","                    if phase == 'train':\n","                        loss.backward()\n","                        optimizer.step()\n","\n","                # statistics\n","                running_loss += loss.item() * inputs.size(0)\n","                running_corrects += torch.sum(preds == labels.data)\n","                running_preds.extend(preds.cpu().numpy())\n","                running_labels.extend(labels.cpu().numpy())\n","\n","            epoch_loss = running_loss / dataset_sizes[phase]\n","            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n","            epoch_acc = epoch_acc.cpu().numpy()  # Move tensor to CPU and convert to numpy(debug)\n","            epoch_f1_score = f1_score(running_labels, running_preds, average='macro')\n","            epoch_f1_score = torch.tensor(epoch_f1_score).cpu().numpy()  # Move tensor to CPU and convert to numpy(debug)\n","\n","            if phase == 'train':\n","                train_losses.append(epoch_loss)\n","                train_accuracies.append(epoch_acc)\n","                train_f1_scores.append(epoch_f1_score)\n","            else:\n","                val_losses.append(epoch_loss)\n","                val_accuracies.append(epoch_acc)\n","                val_f1_scores.append(epoch_f1_score)\n","\n","            print('{} Loss: {:.5f}  Acc: {:.5f}   F1_Score: {:.5f}'.format(\n","                phase, epoch_loss, epoch_acc, epoch_f1_score))\n","\n","            # deep copy the model\n","            if phase == 'val' and epoch_acc > best_acc:\n","                best_acc = epoch_acc\n","                best_model_wts = copy.deepcopy(model.state_dict())\n","                best_preds = running_preds\n","                best_labels = running_labels\n","\n","        print()\n","\n","    time_elapsed = time.time() - since\n","    print('Training complete in {:.0f}m {:.0f}s'.format(\n","        time_elapsed // 60, time_elapsed % 60))\n","    print('Best val Acc: {:5f}'.format(best_acc))\n","\n","    # load best model weights\n","    model.load_state_dict(best_model_wts)\n","\n","    return model, best_preds, best_labels\n","\n","\n","\n","# [I 2023-07-18 19:35:10,874] Trial 49 finished with value: 0.9851851851851853\n","# and parameters: {'lr': 0.00015305939437836783, 'optimizer': 'RMSprop', 'weight_decay': 2.0510689201040282e-10}. Best is trial 49 with value: 0.9851851851851853.\n","# Use the best hyperparameters to train the model\n","best_lr = 0.00001504939437836783  # 0.00001104939437836783\n","# 0.00001705939437836783 Best val Acc: 0.955556\n","# 0.00001103939437836783 Best val Acc: 0.955556\n","# 0.00001104939437836783 Best val Acc: 0.959259\n","# 0.00001104939637836783 Best val Acc: 0.970370\n","best_optimizer_name = 'RMSprop'\n","best_weight_decay = 2.0510689201040282e-10\n","\n","# Load a pretrained model(EfficientNet)\n","model = timm.create_model('tf_efficientnet_b5_ns', pretrained=True)\n","num_ftrs = model.classifier.in_features\n","model.classifier = nn.Linear(num_ftrs, 5)  # Change the classifier layer to 5\n","model = model.to(device)\n","\n","criterion = nn.CrossEntropyLoss()\n","\n","# Use the best optimizer\n","if best_optimizer_name == 'SGD':\n","    optimizer = optim.SGD(model.parameters(), lr=best_lr, weight_decay=best_weight_decay)\n","elif best_optimizer_name == 'Adam':\n","    optimizer = optim.Adam(model.parameters(), lr=best_lr, weight_decay=best_weight_decay)\n","else:\n","    optimizer = optim.RMSprop(model.parameters(), lr=best_lr, weight_decay=best_weight_decay)\n","\n","\n","# Train and evaluate the best model\n","model, best_preds, best_labels = train_model(model, criterion, optimizer, num_epochs=25)\n","\n","\n","\n","\n","\n","\n","\n","\n","\n","# Define a function to apply Integrated Gradients to a given image\n","def apply_integrated_gradients(img, label):\n","\n","\n","    # Add batch dimension and move to device\n","    img = img.unsqueeze(0).to(device)\n","\n","    # Set the model to evaluation mode and get the output\n","    model.eval()\n","    out = model(img)\n","\n","    # Initialize IntegratedGradients object\n","    ig = IntegratedGradients(model)\n","\n","    # Compute attribution scores\n","    attributions_ig = ig.attribute(img, target=label)\n","\n","    # Plot original image and heatmap\n","    fig, axarr = plt.subplots(nrows=1, ncols=2)\n","    img = img.squeeze().cpu().numpy().transpose(1, 2, 0)  # Squeeze before transpose\n","    img = (img - img.min()) / (img.max() - img.min())\n","    axarr[0].imshow(img)\n","    attributions_ig = attributions_ig.squeeze().cpu().numpy().transpose(1, 2, 0)\n","    attributions_ig = (attributions_ig - attributions_ig.min()) / (attributions_ig.max() - attributions_ig.min())\n","    axarr[1].imshow(attributions_ig)\n","    plt.show()\n","\n","# Define a function to apply GradCAM to a given image\n","def apply_gradcam(img, label):\n","    # Add batch dimension and move to device\n","    img = img.unsqueeze(0).to(device)\n","\n","    # Set the model to evaluation mode and get the output\n","    model.eval()\n","    out = model(img)\n","\n","    # Initialize GradCAM\n","    gradcam = GradCAM(model, model.conv_head)\n","\n","    # Get the GradCAM heatmap\n","    mask, _ = gradcam(img)\n","    heatmap, result = visualize_cam(mask, img)\n","\n","    # Plot original image and heatmap\n","    fig, axarr = plt.subplots(nrows=1, ncols=2)\n","    img = img.squeeze().cpu().numpy().transpose(1, 2, 0)  # Squeeze before transpose\n","    img = (img - img.min()) / (img.max() - img.min())\n","    axarr[0].imshow(img)\n","    heatmap = heatmap.squeeze().cpu().numpy().transpose(1, 2, 0)  # Squeeze and transpose before displaying\n","    axarr[1].imshow(heatmap)\n","    plt.show()\n","\n","\n","def visualize_misclassified_images(model, dataloader, max_images):\n","    misclassified_images = 0\n","    # 定义标签映射\n","    label_mapping = {0: 'Bowline', 1: 'Clove_Hitch', 2: 'Figure_8_Knot', 3: 'Overhand', 4: 'Reef'}\n","\n","    # Go through the data\n","    for inputs, labels in dataloader:\n","        inputs, labels = inputs.to(device), labels.to(device)\n","\n","        # Set the model to evaluation mode and get the output\n","        model.eval()\n","        outputs = model(inputs)\n","\n","        # Get the predictions\n","        _, preds = torch.max(outputs, 1)\n","\n","        # Find misclassified images\n","        misclassified_mask = preds != labels\n","        misclassified_inputs = inputs[misclassified_mask]\n","        misclassified_labels = labels[misclassified_mask]\n","\n","        # Track misclassified images in current batch\n","        misclassified_images_batch = 0\n","\n","        for img, label in zip(misclassified_inputs, misclassified_labels):\n","            print(f\"Misclassified image {misclassified_images + 1}:\")\n","            # 使用映射将标签数字转换为字符串\n","            print(f\"True label: {label_mapping[label.item()]}\")\n","            print(f\"Predicted label: {label_mapping[preds[misclassified_mask][misclassified_images_batch].item()]}\")\n","\n","            # Apply Integrated Gradients and visualize\n","            print(\"Integrated Gradients visualization:\")\n","            apply_integrated_gradients(img, label.item())\n","\n","            # Apply GradCAM and visualize\n","            print(\"GradCAM visualization:\")\n","            apply_gradcam(img, label.item())\n","\n","            misclassified_images += 1\n","            misclassified_images_batch += 1\n","\n","            # Stop if we have visualized enough images\n","            if misclassified_images >= max_images:\n","                return\n","\n","\n","\n","# Apply the function to our data\n","visualize_misclassified_images(model, val_dataloader, max_images=100)\n"]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[],"authorship_tag":"ABX9TyN+C2uKBX0x3h/gF8z+Vv9/"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}